# -*- coding: utf-8 -*-
"""PCP_PoC_v5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Hgc6xIP8pdq3Ny2rvKEn6W1HA38GoCke
"""

# Commented out IPython magic to ensure Python compatibility.
# Import packages

import pandas as pd
import numpy as np
import tensorflow as tf

from scipy.optimize import minimize
from scipy.stats import norm

import matplotlib.pyplot as plt
# %matplotlib inline

from datetime import datetime

import time

import torch
import torch.optim as optim

import warnings
warnings.filterwarnings('ignore')

from google.colab import drive
drive.mount('/content/drive')

# Carica dati stock

df = pd.read_csv(r'/content/drive/MyDrive/20230202_ENI.csv', header=None)
df= df.iloc[:,1]

S0 = df[0]      # initial stock price
mu = 0.05     # drift
sigma = 0.25  # volatility
r = 0.03      # risk-free rate

M = 30/365         # maturity
T = 30        # number of time steps

N_MC = 1000    # number of paths

delta_t = M / T                # time interval
gamma = np.exp(- r * delta_t)  # discount factor

# make two datasets 
# the second dataset will be needed below for Double Q-Learning

starttime = time.time()

# stock price
S = pd.DataFrame([], index=range(1, N_MC+1), columns=range(T+1))
S.loc[:,0] = S0

S_1 = pd.DataFrame([], index=range(1, N_MC+1), columns=range(T+1))
S_1.loc[:,0] = S0

# standard normal random numbers
RN = pd.DataFrame(np.random.randn(N_MC,T), index=range(1, N_MC+1), columns=range(1, T+1))
#RN_1 = pd.DataFrame(np.random.randn(N_MC,T), index=range(1, N_MC+1), columns=range(1, T+1))

for t in range(1, T+1):
    S.loc[:,t] = S.loc[:,t-1] * np.exp((mu - 1/2 * sigma**2) * delta_t + sigma * np.sqrt(delta_t) * RN.loc[:,t])
    S_1.loc[:,t] = S_1.loc[:,t-1] * np.exp((mu - 1/2 * sigma**2) * delta_t + sigma * np.sqrt(delta_t) * RN.loc[:,t])

delta_S = S.loc[:,1:T].values - np.exp(r * delta_t) * S.loc[:,0:T-1]
delta_S_hat = delta_S.apply(lambda x: x - np.mean(x), axis=0)

delta_S_1 = S_1.loc[:,1:T].values - np.exp(r * delta_t) * S_1.loc[:,0:T-1]
delta_S_hat_1 = delta_S_1.apply(lambda x: x - np.mean(x), axis=0)

# state variable
X = - (mu - 1/2 * sigma**2) * np.arange(T+1) * delta_t + np.log(S)   # delta_t here is due to their conventions
X_1 = - (mu - 1/2 * sigma**2) * np.arange(T+1) * delta_t + np.log(S_1) 

endtime = time.time()
print('\nTime Cost:', endtime - starttime, 'seconds')

# plot 10 paths
step_size = N_MC // 10
idx_plot = np.arange(step_size, N_MC, step_size)
plt.plot(S.T.iloc[:, idx_plot])
plt.xlabel('Time Steps')
plt.title('Stock Price Sample Paths')
plt.show()

plt.plot(X.T.iloc[:, idx_plot])
plt.xlabel('Time Steps')
plt.ylabel('State Variable');

# Trasforma dataframe in array

df_np = df.to_numpy()

# TRASFORMARE PREZZI ENI DA S-->X (STANDARDIZZA)

sigma_td = np.std(df_np)
mu_td = np.mean(df_np)
delta_t_td = 1/(252*690)
T_td=df.shape[0]

X_td = - (mu_td - 1/2 * sigma_td**2) * np.arange(T_td) * delta_t_td + np.log(df_np)

min_td=np.min(X_td)
max_td=np.max(X_td)

!pip install bspline

import bspline
import bspline.splinelab as splinelab

X_min_sim = np.min(np.min(X))
X_max_sim = np.max(np.max(X))

X_min=min(min_td, X_min_sim)
X_max=max(max_td, X_max_sim)

print('X.shape = ', X.shape)
print('X_min, X_max = ', X_min, X_max)

p = 5 # order of spline (as-is; 3: cubic, 4: B-spline?)
ncolloc = 12
tau = np.linspace(X_min, X_max, ncolloc)  # These are the sites to which we would like to interpolate

# k is a knot vector that adds endpoints repeats as appropriate for a spline of order p
# To get meaninful results, one should have ncolloc >= p+1
k = splinelab.aptknt(tau, p) 
                             
# Spline basis of order p on knots k
basis = bspline.Bspline(k, p)        
f = plt.figure()

print('Number of points k = ', len(k))
basis.plot()
plt.savefig('Basis_functions.png', dpi=600)

def terminal_payoff_put(ST, K):
    # ST   final stock price
    # K    strike
    payoff = max(K - ST, 0)
    return payoff

def terminal_payoff_call(ST, K):
    # ST   final stock price
    # K    strike
    payoff = max(ST-K, 0)
    return payoff

num_t_steps = T + 1
num_basis =  ncolloc 

data_mat_t = np.zeros((num_t_steps, N_MC,num_basis ))
data_mat_t_1 = np.zeros((num_t_steps, N_MC,num_basis ))

print('num_basis = ', num_basis)
print('dim data_mat_t = ', data_mat_t.shape)

t_0 = time.time()

# fill it 
for i in np.arange(num_t_steps):
    x = X.values[:,i]
    x_1 = X_1.values[:,i]
    data_mat_t[i,:,:] = np.array([ basis(i) for i in x ])
    data_mat_t_1[i,:,:] = np.array([ basis(i) for i in x_1 ])
    
t_end = time.time()
print('Computational time:', t_end - t_0, 'seconds')

# save these data matrices for future re-use
np.save('data_mat_m=r_A_%d' % N_MC, data_mat_t)
np.save('data_mat_m=r_B_%d' % N_MC, data_mat_t_1)

risk_lambda = 0.1 # risk aversion parameter
K = 14 # strike

# functions to compute optimal hedges
def function_A_vec(t,delta_S_hat,data_mat,reg_param):
    # Compute the matrix A_{nm} from Eq. (52) (with a regularization!)
    X_mat = data_mat_t[t,:,:]
    num_basis_funcs = X_mat.shape[1]
    this_dS = delta_S_hat.loc[:,t].values
    hat_dS2 = (this_dS**2).reshape(-1,1)    
    A_mat = np.dot(X_mat.T, X_mat * hat_dS2) + reg_param * np.eye(num_basis_funcs)
    return A_mat
        
def function_B_vec(t, Pi_hat, delta_S=delta_S, delta_S_hat=delta_S_hat, S=S, data_mat=data_mat_t,
                  gamma=gamma,risk_lambda=risk_lambda):

    drift_hedge = (np.exp(mu * delta_t) - np.exp(r * delta_t)) * delta_S.loc[:, t]
    
    #Importa risk-pure hedging o risk sensitive hedging
    coef = 1.0/(2 * gamma * risk_lambda)
    
    # override it by zero to have pure risk hedge
    #coef = 0
    #tmp =  Pi_hat.loc[:,t+1] * delta_S_hat.loc[:,t] + coef * (np.exp(mu*delta_t) - np.exp(r*delta_t))* delta_S.loc[:,t]

    tmp = Pi_hat.loc[:, t + 1] * delta_S_hat.loc[:, t] + coef * drift_hedge
    
    X_mat = data_mat_t[t,:,:]  # matrix of dimension N_MC x num_basis
    
    B = np.dot(X_mat.T, tmp)  
    return B

starttime = time.time()

# portfolio value
Pi = pd.DataFrame([], index=range(1, N_MC+1), columns=range(T+1))
Pi.iloc[:,-1] = S.iloc[:,-1].apply(lambda x: terminal_payoff_call(x, K))

Pi_hat = pd.DataFrame([], index=range(1, N_MC+1), columns=range(T+1))
Pi_hat.iloc[:,-1] = Pi.iloc[:,-1] - np.mean(Pi.iloc[:,-1])

# optimal hedge
a = pd.DataFrame([], index=range(1, N_MC+1), columns=range(T+1))
a.iloc[:,-1] = 0

# twin variables for the second dataset
# portfolio value
Pi_1 = pd.DataFrame([], index=range(1, N_MC+1), columns=range(T+1))
Pi_1.iloc[:,-1] = S_1.iloc[:,-1].apply(lambda x: terminal_payoff_put(x, K))

Pi_hat_1 = pd.DataFrame([], index=range(1, N_MC+1), columns=range(T+1))
Pi_hat_1.iloc[:,-1] = Pi_1.iloc[:,-1] - np.mean(Pi_1.iloc[:,-1])

# optimal hedge
a_1 = pd.DataFrame([], index=range(1, N_MC+1), columns=range(T+1))
a_1.iloc[:,-1] = 0

reg_param = 1e-3

for t in range(T-1, -1, -1):
    A_mat = function_A_vec(t, delta_S_hat, data_mat_t, reg_param)
    B_vec = function_B_vec(t, Pi_hat)

    # print ('t =  A_mat.shape = B_vec.shape = ', t, A_mat.shape, B_vec.shape)
    phi = np.dot(np.linalg.inv(A_mat), B_vec)

    a.loc[:,t] = np.dot(data_mat_t[t,:,:],phi)
    Pi.loc[:,t] = gamma * (Pi.loc[:,t+1] - a.loc[:,t] * delta_S.loc[:,t])
    Pi_hat.loc[:,t] = Pi.loc[:,t] - np.mean(Pi.loc[:,t])

    # and the same for the twin dataset:
    A_mat_1 = function_A_vec(t,delta_S_hat_1,data_mat_t_1,reg_param)
    B_vec_1 = function_B_vec(t, Pi_hat_1)
    phi_1 = np.dot(np.linalg.inv(A_mat_1), B_vec_1)
    a_1.loc[:,t] = np.dot(data_mat_t_1[t,:,:],phi_1)
    Pi_1.loc[:,t] = gamma * (Pi_1.loc[:,t+1] - a_1.loc[:,t] * delta_S_1.loc[:,t])
    Pi_hat_1.loc[:,t] = Pi_1.loc[:,t] - np.mean(Pi_1.loc[:,t])

a = a.astype('float')
Pi = Pi.astype('float')
Pi_hat = Pi_hat.astype('float')

a_1 = a_1.astype('float')
Pi_1 = Pi_1.astype('float')
Pi_hat_1 = Pi_hat_1.astype('float')

endtime = time.time()
print('Computational time:', endtime - starttime, 'seconds')

# plot 10 paths
plt.plot(a.T.iloc[:,idx_plot])
plt.xlabel('Time Steps')
plt.title('Optimal Hedge')
plt.show()

plt.plot(Pi.T.iloc[:,idx_plot])
plt.xlabel('Time Steps')
plt.title('Portfolio Value');

# # CREA SEGNALI

# SCOMPONI X SECONDO B-SPLINE
X_td_splined = np.array([basis(i) for i in X_td])

# CALCOLA IL SEGNALE PER LA CALL: MATRICE * PHI
z_call =np.dot(X_td_splined, phi)

# CALCOLA IL SEGNALE PER LA PUT: MATRICE * PHI_1
z_put = np.dot(X_td_splined, phi_1)

# SOMMARE I SEGNALI PER OTTENERE 690 * 1 SEGNALI
z= z_call + z_put

z = np.reshape(z, (690, 1))

#
#
#
#       INIZIO G - LEARNING
#
#
#

# The Black-Scholes prices
def bs_put(S0=S0, K=K, r=r, sigma=sigma, T=M):
    d1 = (np.log(S0/K) + (r + 1/2 * sigma**2) * (T)) / sigma / np.sqrt(T)
    d2 = (np.log(S0/K) + (r - 1/2 * sigma**2) * (T)) / sigma / np.sqrt(T)
    price = K * np.exp(-r * (T)) * norm.cdf(-d2) - S0 * norm.cdf(-d1)
    return price

def bs_call(S0=S0, K=K, r=r, sigma=sigma, T=M):
    d1 = (np.log(S0/K) + (r + 1/2 * sigma**2) * (T)) / sigma / np.sqrt(T)
    d2 = (np.log(S0/K) + (r - 1/2 * sigma**2) * (T)) / sigma / np.sqrt(T)
    price = S0 * norm.cdf(d1) - K * np.exp(-r * (T)) * norm.cdf(d2)
    return price

# Generate realized returns and realized asset values by simulating from a one-factor model 
# with time-dependent expected returns

vol_stock=0.3
strike= 14

num_steps=df.shape[0]
num_fin_opt=2

fin_opt_returns = np.zeros((num_steps, num_fin_opt))
fin_opt_vals = np.zeros((num_steps, num_fin_opt))

idiosync_vol =  0.0001 

for t in range(num_steps):
    
    stock_price = df.iloc[t]
    rand_norm = np.random.randn(num_fin_opt)

    fin_opt_vals[t,0] = bs_call(stock_price, strike, r, vol_stock, M)+rand_norm[0]*idiosync_vol
    fin_opt_vals[t,1] = bs_put(stock_price, strike, r, vol_stock, M)+rand_norm[1]*idiosync_vol

df_fin_opt = pd.DataFrame(data=fin_opt_vals)

df_cap=pd.concat([df, df_fin_opt], axis=1)

df_cap.columns = ['stock', 'call', 'put']

df_cap.head()

df_ret= df_cap.pct_change().shift(-1)

df_ret.iloc[-1,:]=0

df_ret.tail()

#
#
#
#
#  SIAMO QUI
#
#
#
#
#

"""**G-learning for Put Call Parity trading**

The purpose of this notebook is to realize a model-free batch (offline) policy estimation for an arbitrage trading strategy based on put call parity and G-learning.

La funzione risolve un errore di OpenMP con TensorFlow/Keras. Si imposta una variabile d'ambiente per consentire la duplicazione delle librerie in memoria. Ciò consente di evitare un errore di blocco nell'addestramento del modello.
"""

# Commented out IPython magic to ensure Python compatibility.
# %env KMP_DUPLICATE_LIB_OK=TRUE

"""La linea di codice device = 'cuda' if torch.cuda.is_available() else 'cpu' in Python imposta la variabile device in base alla disponibilità di una GPU (scheda grafica) nel sistema."""

device = 'cuda' if torch.cuda.is_available() else 'cpu'

device

# Define the G-learning portfolio optimization class
class G_learning_portfolio_opt:
    
    def __init__(self, 
                 num_steps,           # numero di step della policy
                 params,              # parametri del G-learner
                                      # reward_params=[lambd, eta]
                                      # lambd: avversione al rischio
                                      # eta: parametro da applicare ai costi di transazione
                 W,
                 beta,                # coefficiente regola la penalizzazione rispetto alla reference policy
                 gamma,               # fattore di attualizzazione intertemporale "non finanziario" 
                 num_risky_assets,    # numero di strumenti diversi dal risk free
                 exp_returns,         # matrice dei rendimenti degli n asset rischiosi
                 Sigma_r,             # covariance matrix of returns of risky assets
                 x_vals_init,         # valore iniziale in euro della posizione negli n asset rischiosi
                 signals,             # la matrice dei segnali
                 riskfree_rate,
                 use_for_WM = True
                 ):

                
        self.num_steps = num_steps
        self.num_risky_assets = num_risky_assets
        self.r_f = riskfree_rate
        self.beta = torch.tensor(beta, requires_grad=False, dtype=torch.float64)
        self.z_np = signals
        self.num_assets = num_risky_assets + 1 
        
        self.lambd = torch.tensor(params[0], requires_grad=False, dtype=torch.float64)
        self.eta = torch.tensor(params[1], requires_grad=False, dtype=torch.float64)
        self.C = torch.eye(self.num_assets, dtype=torch.float64)
        self.gamma = gamma
        self.use_for_WM = use_for_WM
        
        self.num_signals=signals.shape[1]
        self.W=W
        self.z = torch.tensor(self.z_np, dtype=torch.float64, requires_grad=False)

        assert exp_returns.shape[0] == self.num_steps
        assert Sigma_r.shape[0] == Sigma_r.shape[1]
        assert Sigma_r.shape[0] == num_risky_assets # self.num_assets
        
        self.Sigma_r_np = Sigma_r # array of shape num_stocks x num_stocks
        self.reg_mat = 1e-3*torch.eye(self.num_assets, dtype=torch.float64)  # 
        
        # arrays of returns for all assets including the risk-free asset
        # array of shape num_steps x (num_stocks + 1) 
        self.exp_returns_np = np.hstack((self.r_f * np.ones(self.num_steps).reshape((-1,1)), exp_returns))
                                      
        # make block-matrix Sigma_r_tilde with Sigma_r_tilde[0,0] = 0, and equity correlation matrix inside
        self.Sigma_r_tilde_np = np.zeros((self.num_assets, self.num_assets))
        self.Sigma_r_tilde_np[1:,1:] = self.Sigma_r_np

        # make Torch tensors

        self.exp_returns = torch.tensor(self.exp_returns_np,requires_grad=False, dtype=torch.float64)
        self.Sigma_r = torch.tensor(Sigma_r,requires_grad=False, dtype=torch.float64)
        self.Sigma_r_tilde = torch.tensor(self.Sigma_r_tilde_np,requires_grad=False, dtype=torch.float64)
        
        # asset holding values for all times. Initialize with initial values, values for the future times will be expected values 
        
        self.x_vals_np = np.zeros((self.num_steps, self.num_assets))
        self.x_vals_np[0,:] = x_vals_init 
        
        # Torch tensor
        self.x_vals = torch.tensor(self.x_vals_np, dtype=torch.float64)
                
        # allocate memory for coefficients of R-, F- and G-functions        
        self.F_xx = torch.zeros(self.num_steps, self.num_assets, self.num_assets, dtype=torch.float64,
                                requires_grad=False)
        self.F_x = torch.zeros(self.num_steps, self.num_assets, dtype=torch.float64,
                               requires_grad=False)
        self.F_0 = torch.zeros(self.num_steps,dtype=torch.float64,requires_grad=False)
        
        self.Q_xx = torch.zeros(self.num_steps, self.num_assets, self.num_assets,dtype=torch.float64,
                                requires_grad=False)
        self.Q_uu = torch.zeros(self.num_steps, self.num_assets, self.num_assets,dtype=torch.float64,
                                requires_grad=False)
        self.Q_ux = torch.zeros(self.num_steps, self.num_assets, self.num_assets,dtype=torch.float64,
                                requires_grad=False)
        self.Q_x = torch.zeros(self.num_steps, self.num_assets,dtype=torch.float64,requires_grad=False)
        self.Q_u = torch.zeros(self.num_steps, self.num_assets,dtype=torch.float64,requires_grad=False)
        self.Q_0 = torch.zeros(self.num_steps,dtype=torch.float64,requires_grad=False)
        
        self.R_xx = torch.zeros(self.num_steps, self.num_assets, self.num_assets,dtype=torch.float64,
                                requires_grad=False)
        self.R_uu = torch.zeros(self.num_steps, self.num_assets, self.num_assets,dtype=torch.float64,
                                requires_grad=False)
        self.R_ux = torch.zeros(self.num_steps, self.num_assets, self.num_assets,dtype=torch.float64,
                                requires_grad=False)
        self.R_x = torch.zeros(self.num_steps, self.num_assets,dtype=torch.float64,requires_grad=False)
        self.R_u = torch.zeros(self.num_steps, self.num_assets,dtype=torch.float64,requires_grad=False)
        self.R_0 = torch.zeros(self.num_steps,dtype=torch.float64,requires_grad=False)

        
        self.reset_prior_policy()
        
        # expected portfolio values for all times
        self.expected_portf_val = torch.zeros(self.num_steps,dtype=torch.float64)
        
        # the first value is the sum of initial position values
        self.expected_portf_val[0] = self.x_vals[0,:].sum()

    def reset_prior_policy(self):
        # initialize time-dependent parameters of prior policy 
        self.u_bar_prior = torch.zeros(self.num_steps,self.num_assets,requires_grad=False,
                                       dtype=torch.float64)
        self.v_bar_prior =  torch.zeros(self.num_steps, self.num_assets, self.num_assets,requires_grad=False,
                                        dtype=torch.float64)
        self.Sigma_prior =  torch.zeros(self.num_steps, self.num_assets, self.num_assets,requires_grad=False,
                                        dtype=torch.float64)
        self.Sigma_prior_inv = torch.zeros(self.num_steps, self.num_assets, self.num_assets,requires_grad=False,
                                        dtype=torch.float64)
        
        
        # make each time elements of v_bar_prior and Sigma_prior proportional to the unit matrix
        for t in range(self.num_steps):
            self.v_bar_prior[t,:,:] = 0.1 * torch.eye(self.num_assets)
            self.Sigma_prior[t,:,:] = 0.1 * torch.eye(self.num_assets).clone()
            self.Sigma_prior_inv[t,:,:] = 10.0 * torch.eye(self.num_assets).clone() # np.linalg.inv(self.Sigma_prior[t,:,:])
    
    
    def compute_reward_fun(self):
        """
        Compute coefficients R_xx, R_ux, etc. for all steps
        """
        C=torch.eye(self.num_assets,dtype=torch.float64)
        
        for t in range(0, self.num_steps):
            
            one_plus_exp_ret = torch.ones(self.num_assets,dtype=torch.float64) + self.exp_returns[t,:]
            # benchmark_portf = self.benchmark_portf[t]
            Sigma_hat = self.Sigma_r_tilde + torch.ger(one_plus_exp_ret, one_plus_exp_ret)
            

            # LA VARIANZA INIZIALE DEVE ESSERE COERENTE CON DELTA_T PER LA FORMULA DI SIGMA_HAT
            
            one_plus_exp_ret_by_one = torch.ger(one_plus_exp_ret,torch.ones(self.num_assets,dtype=torch.float64))
            one_plus_exp_ret_by_one_T = one_plus_exp_ret_by_one.t()     
            #one_one_T_mat = torch.ones(self.num_assets,self.num_assets)
            
            self.R_xx[t,:,:] = -self.lambd*Sigma_hat.clone()
            self.R_ux[t,:,:] = -2*self.lambd.clone()*Sigma_hat.clone()
            self.R_uu[t,:,:] = -self.eta*C
            self.R_x[t,:] =  self.W.mv(self.z[t,:])
            self.R_u[t,:] = self.W.mv(self.z[t,:])
            self.R_0[t] = 0
                
         
    def set_terminal_conditions(self):
        """
        set the terminal condition for the F-function
        """
        
        # the auxiliary quantity to perform matrix calculations
        one_plus_exp_ret = torch.ones(self.num_assets,dtype=torch.float64) + self.exp_returns[-1,:]
        
        # Compute the reward function for all steps (only the last step is needed for this functions, while 
        # values for other time steps will be used in other functions)
        self.compute_reward_fun()
        
        if self.use_for_WM:

          
            Sigma_hat = self.Sigma_r_tilde + torch.ger(one_plus_exp_ret, one_plus_exp_ret)
            Sigma_hat_inv = torch.inverse(Sigma_hat + self.reg_mat)
            
            Sigma_tilde = Sigma_hat + (1/self.lambd.clone())*self.C.clone()
            Sigma_tilde_inv = torch.inverse(Sigma_tilde + self.reg_mat)
            
            #Sigma_hat_sigma_tilde = Sigma_hat.mm(Sigma_tilde)
            #Sigma_tilde_inv_sig_hat = Sigma_tilde_inv.mm(Sigma_hat)
            #Sigma_tilde_sigma_hat = Sigma_tilde.mm(Sigma_hat)
            
            #Sigma_hat_Sigma_tilde_inv = Sigma_hat.mm(Sigma_tilde_inv)
            #Sigma_3_plus_omega = self.lambd.clone()*Sigma_tilde_inv.mm(Sigma_hat_Sigma_tilde_inv) + self.C.clone()    
                             
            one_plus_exp_ret_by_one = torch.ger(one_plus_exp_ret,torch.ones(self.num_assets,dtype=torch.float64))
            one_plus_exp_ret_by_one_T = one_plus_exp_ret_by_one.t()     
            #one_one_T_mat = torch.ones(self.num_assets,self.num_assets)
            
            #Sigma_tilde_inv_t_R_ux = Sigma_tilde_inv.t().mm(self.R_ux[-1,:,:].clone())
            #Sigma_tilde_inv_t_R_uu = Sigma_tilde_inv.t().mm(self.R_uu[-1,:,:].clone())
            #Sigma_tilde_inv_t_R_u = Sigma_tilde_inv.t().mv(self.R_u[-1,:].clone())
            
            #Sigma_tilde_inv_R_u = Sigma_tilde_inv.mv(self.R_u[-1,:].clone())
            #Sigma_tilde_inv_R_ux = Sigma_tilde_inv.mm(self.R_ux[-1,:,:].clone())
            #Sigma_tilde_inv_t_R_uu = Sigma_tilde_inv.mm(self.R_uu[-1,:,:].clone())
            
            

            # though the action at the last step is deterministic, we can feed 
            # parameters of the prior with these values                     
              
            self.u_bar_prior[-1,:]   = (1/(2 * self.lambd.clone()))* Sigma_tilde_inv.clone().mv(self.R_u[-1,:].clone())
            self.v_bar_prior[-1,:,:] = (1/(2 * self.lambd.clone()))* Sigma_tilde_inv.clone().mm(self.R_ux[-1,:,:].clone())    
                
            # First compute the coefficients of the reward function F at the last step:        
            # F_xx                 
            self.F_xx[-1,:,:] = self.R_uu[t,:,:]
            
            # F_x                    
            self.F_x[-1,:] = 0
            
            # F_0   
            self.F_0[-1] = 0
            
            # for the Q-function at the last step:
            self.Q_xx[-1,:,:] = self.R_xx[-1,:,:].clone()
            self.Q_ux[-1,:,:] = self.R_ux[-1,:,:].clone()
            self.Q_uu[-1,:,:] = self.R_uu[-1,:,:].clone()
            self.Q_u[-1,:] = self.R_u[-1,:].clone()
            self.Q_x[-1,:] = self.R_x[-1,:].clone()
            self.Q_0[-1] = self.R_0[-1].clone()
            
    def G_learning(self, err_tol, max_iter):
        """
        find the optimal policy for the time dependent policy
        
        """   
        print('Doing G-learning, it may take a few seconds...')
        
        # set terminal conditions
        self.set_terminal_conditions()
        
        # allocate iteration numbers for all steps
        self.iter_counts = np.zeros(self.num_steps)
        
        # iterate over time steps backward
        for t in range(self.num_steps-2,-1,-1):
            self.step_G_learning(t, err_tol, max_iter)
            
    def step_G_learning(self, t, err_tol, max_iter):
        """
        Perform one step of backward iteration for G-learning self-consistent equations
        This should start from step t = num_steps - 2 (i.e. from a step that is before the last one)
        """
            
        # make matrix Sigma_hat_t        
        one_plus_exp_ret = torch.ones(self.num_assets,dtype=torch.float64) + self.exp_returns[t,:]
        Sigma_hat_t = self.Sigma_r_tilde # + torch.ger(one_plus_exp_ret, one_plus_exp_ret)
        
        # matrix A_t = diag(1 + r_f + W*z)
        A_t = torch.diag(torch.ones(self.num_assets,dtype=torch.float64) + self.r_f+self.W.mv(self.z[t,:]))
                    
        # update parameters of Q_function using next-step F-function values
        self.update_Q_params(t, A_t,Sigma_hat_t)
             
        # iterate between policy evaluation and policy improvement  
        while self.iter_counts[t] < max_iter:
                
            curr_u_bar_prior = self.u_bar_prior[t,:].clone()  
            curr_v_bar_prior = self.v_bar_prior[t,:,:].clone()     
                
            # compute parameters of F-function for this step from parameters of Q-function
            self.update_F_params(t) 
              
            # Policy iteration step: update parameters of the prior policy distribution
            # with given Q- and F-function parameters
            self.update_policy_params(t)    
            
            # difference between the current value of u_bar_prior and the previous one
            err_u_bar = torch.sum((curr_u_bar_prior - self.u_bar_prior[t,:])**2)
            
            # divide by num_assets in err_v_bar to get both errors on a comparable scale
            err_v_bar = (1/self.num_assets)*torch.sum((curr_v_bar_prior - self.v_bar_prior[t,:,:])**2)
            
            # choose the difference from the previous iteration as the maximum of the two errors
            tol = torch.max(err_u_bar, err_v_bar)  # tol = 0.5*(err_u_bar + err_v_bar)
            
            self.iter_counts[t] += 1
            # Repeat the calculation of Q- and F-values
            if tol <= err_tol:
                break
                
    def update_Q_params(self,t, A_t,Sigma_hat_t):
        """
        update the current (time-t) parameters of Q-function from (t+1)-parameters of F-function
        """ 
                
        ones = torch.ones(self.num_assets,dtype=torch.float64)    
        one_plus_exp_ret = torch.ones(self.num_assets,dtype=torch.float64) + self.exp_returns[t,:]
    
        self.Q_xx[t,:,:] = (self.R_xx[t,:,:].clone() 
                            + self.gamma *( (A_t.clone().mm(self.F_xx[t+1,:,:].clone())).mm(A_t.clone())  
                                           + self.Sigma_r_tilde.clone() * self.F_xx[t+1,:,:].clone() ) )


        self.Q_ux[t,:,:] = (self.R_ux[t,:,:].clone() 
                            + 2 * self.gamma *( (A_t.clone().mm(self.F_xx[t+1,:,:].clone())).mm(A_t.clone())  
                                           + self.Sigma_r_tilde.clone() * self.F_xx[t+1,:,:].clone() ) 
                           )
    
        self.Q_uu[t,:,:] = (self.R_uu[t,:,:].clone()  
                            + self.R_xx[t,:,:].clone()
                            + self.gamma *( (A_t.clone().mm(self.F_xx[t+1,:,:].clone())).mm(A_t.clone())  
                                           + self.Sigma_r_tilde.clone() * self.F_xx[t+1,:,:].clone() ) )


        self.Q_x[t,:] = self.R_x[t,:].clone() + self.gamma * A_t.clone().mv(self.F_x[t+1,:].clone()) 
        self.Q_u[t,:] = self.R_u[t,:].clone() + self.gamma * A_t.clone().mv(self.F_x[t+1,:].clone())
        self.Q_0[t]   = self.gamma * self.F_0[t+1].clone()

    def update_F_params(self,t):
        """
        update the current (time-t) parameters of F-function from t-parameters of G-function
        This is a policy evaluation step: it uses the current estimations of the mean parameters of the policy
        
        """
        
        # produce auxiliary parameters U_t, W_t, Sigma_tilde_t

        U_t = (self.beta.clone() * self.Q_ux[t,:,:].clone() 
               + self.Sigma_prior_inv[t,:,:].clone().mm(self.v_bar_prior[t,:,:].clone()))
        W_t = (self.beta.clone() * self.Q_u[t,:].clone() 
               +  self.Sigma_prior_inv[t,:,:].clone().mv(self.u_bar_prior[t,:]).clone())
        Sigma_p_bar =  self.Sigma_prior_inv[t,:,:].clone() - 2 * self.beta.clone() * self.Q_uu[t,:,:].clone()
        Sigma_p_bar_inv = torch.inverse(Sigma_p_bar + self.reg_mat)
        
        # update parameters of F-function
        self.F_xx[t,:,:] = self.Q_xx[t,:,:].clone() + (1/(2*self.beta.clone()))*(U_t.t().mm(Sigma_p_bar_inv.clone().mm(U_t))
                                    - self.v_bar_prior[t,:,:].clone().t().mm(
                                        self.Sigma_prior_inv[t,:,:].clone().mm(self.v_bar_prior[t,:,:].clone())))
        
        
        self.F_x[t,:] = self.Q_x[t,:].clone() + (1/self.beta.clone())*(U_t.mv(Sigma_p_bar_inv.clone().mv(W_t))
                                    - self.v_bar_prior[t,:,:].clone().mv(
                                        self.Sigma_prior_inv[t,:,:].clone().mv(self.u_bar_prior[t,:].clone())))
        
        
        self.F_0[t] = self.Q_0[t].clone() + ( (1/(2*self.beta.clone()))*(W_t.dot(Sigma_p_bar_inv.clone().mv(W_t))
                                    - self.u_bar_prior[t,:].clone().dot(
                                        self.Sigma_prior_inv[t,:,:].clone().mv(self.u_bar_prior[t,:].clone())))
                                    - (1/(2*self.beta.clone())) * (torch.log(torch.det(self.Sigma_prior[t,:,:].clone()+
                                                                              self.reg_mat))
                                                       - torch.log(torch.det(Sigma_p_bar_inv.clone() + self.reg_mat))) )

    def update_policy_params(self,t):
        """
        update parameters of the Gaussian policy using current coefficients of the F- and G-functions
        """
        
        new_Sigma_prior_inv = self.Sigma_prior_inv[t,:,:].clone() - 2 * self.beta.clone() * self.Q_uu[t,:,:].clone()

        Sigma_prior_new = torch.inverse(new_Sigma_prior_inv + self.reg_mat)
        
        # update parameters using the previous value of Sigma_prior_inv
        self.u_bar_prior[t,:] = Sigma_prior_new.mv(self.Sigma_prior_inv[t,:,:].clone().mv(self.u_bar_prior[t,:].clone())
                                              + self.beta.clone() * self.Q_u[t,:].clone())
        
        
        self.v_bar_prior[t,:,:] = Sigma_prior_new.clone().mm(self.Sigma_prior_inv[t,:,:].clone().mm(self.v_bar_prior[t,:,:].clone())
                                              + self.beta.clone() * self.Q_ux[t,:,:].clone())
        
        # and then assign the new inverse covariance for the prior for the next iteration
        self.Sigma_prior[t,:,:] = Sigma_prior_new.clone()
        self.Sigma_prior_inv[t,:,:] = new_Sigma_prior_inv.clone()
        
        # also assign the same values for the previous time step
        if t > 0:
            self.Sigma_prior[t-1,:,:] = self.Sigma_prior[t,:,:].clone()
            self.u_bar_prior[t-1,:] = self.u_bar_prior[t,:].clone()
            self.v_bar_prior[t-1,:,:] = self.v_bar_prior[t,:,:].clone()
            
    def trajs_to_torch_tensors(self,trajs):
        """
        Convert data from a list of lists into Torch tensors
        """
        num_trajs = len(trajs)
        
        self.data_xvals = torch.zeros(num_trajs,self.num_steps,self.num_assets,dtype=torch.float64)
        self.data_uvals = torch.zeros(num_trajs,self.num_steps,self.num_assets,dtype=torch.float64)
            
        for n in range(num_trajs):
            for t in range(self.num_steps):
                self.data_xvals[n,t,:] = torch.tensor(trajs[n][t][0],dtype=torch.float64).clone()
                self.data_uvals[n,t,:] = torch.tensor(trajs[n][t][1],dtype=torch.float64).clone()
                
    def compute_reward_on_traj(self,
                              t,
                              x_t, u_t):
        """
        Given time t and corresponding values of vectors x_t, u_t, compute the total reward for this step
        """
        
        aux_xx = x_t.dot(self.R_xx[t,:,:].clone().mv(x_t))
        aux_ux = u_t.dot(self.R_ux[t,:,:].clone().mv(x_t))
        aux_uu = u_t.dot(self.R_uu[t,:,:].clone().mv(u_t))
        aux_x = x_t.dot(self.R_x[t,:].clone())
        aux_u = u_t.dot(self.R_u[t,:].clone())
        aux_0 = self.R_0[t].clone()
        
        return aux_xx + aux_ux + aux_uu + aux_x + aux_u + aux_0
    
    def compute_G_fun_on_traj(self,
                              t,
                              x_t, u_t):
        """
        Given time t and corresponding values of vectors x_t, u_t, compute the total reward for this step
        """
        
        aux_xx = x_t.dot(self.Q_xx[t,:,:].clone().mv(x_t))
        aux_ux = u_t.dot(self.Q_ux[t,:,:].clone().mv(x_t))
        aux_uu = u_t.dot(self.Q_uu[t,:,:].clone().mv(u_t))
        aux_x = x_t.dot(self.Q_x[t,:].clone())
        aux_u = u_t.dot(self.Q_u[t,:].clone())
        aux_0 = self.Q_0[t].clone()
        
        return aux_xx + aux_ux + aux_uu + aux_x + aux_u + aux_0
    
    def compute_F_fun_on_traj(self,
                              t,
                              x_t):
        """
        Given time t and corresponding values of vectors x_t, u_t, compute the total reward for this step
        """
        
        aux_xx = x_t.dot(self.F_xx[t,:,:].clone().mv(x_t))
        aux_x = x_t.dot(self.F_x[t,:].clone())
        aux_0 = self.F_0[t].clone()
        
        return aux_xx + aux_x + aux_0
                 
    def MaxEntIRL(self,
                  trajs,
                  learning_rate,
                  err_tol, max_iter):
        
        """
        Estimate parameters of the reward function using MaxEnt IRL.
        Inputs:
        
        trajs - a list of trajectories. Each trajectory is a list of state-action pairs, stored as a tuple.
                We assume each trajectory has the same length
        """
        
        # omega is a tunable parameter that determines the cost matrix self.W
        self.W_np = np.zeros((self.z_np.shape[1], self.z_np.shape[0]))
        self.W = torch.tensor(self.W_np, requires_grad=True, dtype=torch.float64)
        
        beta_init = 50 # Beta is fixed and not a learned parameter.
        self.beta = torch.tensor(beta_init, requires_grad=True, dtype=torch.float64)
        
        reward_params =  [self.lambd, self.eta, self.C]
        
       
        print("g learning...")
        self.reset_prior_policy()
        self.G_learning(err_tol, max_iter)
        print("intialize optimizer...")
        optimizer = optim.Adam(self.W, lr=learning_rate)
        print("zero grad...")
        optimizer.zero_grad()
        
        num_trajs = len(trajs)
        print("trajs_to_torch_tensors...")
        
        # fill in Torch tensors for the trajectory data
        self.trajs_to_torch_tensors(trajs)
        print("constructing zero tensors...")   
        self.realized_rewards = torch.zeros(num_trajs,self.num_steps,dtype=torch.float64,requires_grad=True)
        self.realized_cum_rewards = torch.zeros(num_trajs, dtype=torch.float64, requires_grad=True)
        print("constructing zero tensors...")  
        self.realized_G_fun = torch.zeros(num_trajs,self.num_steps,dtype=torch.float64, requires_grad=True)
        self.realized_F_fun = torch.zeros(num_trajs,self.num_steps,dtype=torch.float64, requires_grad=True)
        print("constructing zero tensors...")  
        self.realized_G_fun_cum = torch.zeros(num_trajs,dtype=torch.float64, requires_grad=True)
        self.realized_F_fun_cum = torch.zeros(num_trajs,dtype=torch.float64, requires_grad=True)
        print("done...")  
        
        num_iter_IRL = 3
        
        for i in range(num_iter_IRL):
            print('GIRL iteration = ', i)
    
            self.C = self.eta * torch.eye(self.num_assets,dtype=torch.float64)
    
            for n in range(101):
                if n%100==0:
                    print(n)
                for t in range(self.num_steps):
                    
                    # compute rewards obtained at each step for each trajectory
                    # given the model parameters
                    self.realized_rewards[n,t] = self.compute_reward_on_traj(t,
                                                                self.data_xvals[n,t,:],
                                                                self.data_uvals[n,t,:])
                                                                
            
                    # compute the log-likelihood by looping over trajectories
                    self.realized_G_fun[n,t] = self.compute_G_fun_on_traj(t,
                                                                self.data_xvals[n,t,:],
                                                                self.data_uvals[n,t,:])
                
                
                    self.realized_F_fun[n,t] = self.compute_F_fun_on_traj(t,
                                                                self.data_xvals[n,t,:])
                

                self.realized_cum_rewards[n] = self.realized_rewards[n,:].sum().clone()
                self.realized_G_fun_cum[n] = self.realized_G_fun[n,:].sum().clone()
                self.realized_F_fun_cum[n] = self.realized_F_fun[n,:].sum().clone()
            
            # the negative log-likelihood will not include terms ~ Sigma_p as we do not optimize over its value
            loss = - self.beta.clone()*(self.realized_G_fun_cum.sum().clone() - self.realized_F_fun_cum.sum().clone())
        
            optimizer.zero_grad()
        
            loss.backward() 
        
            optimizer.step()
        
            print('Iteration = ', i)
            print('Loss = ', loss.detach().numpy())
        
           
        print('Done optimizing reward parameters')

"""Compute the empirical correlation matrix using realized returns"""

risky_asset_returns = df_ret.to_numpy()
risky_asset_vals=df_cap.to_numpy()

cov_mat_r = np.cov(risky_asset_returns.T) 
corr_mat_r = np.corrcoef(risky_asset_returns.T) 
print(cov_mat_r.shape)

D, v = np.linalg.eigh(cov_mat_r)

eigenvals = D[::-1]  # put them in a descended order

cov_mat_torch = torch.tensor(cov_mat_r)
torch.pinverse(cov_mat_torch)

"""
Add a riskless bond as one more asset"""

df_cap.head()

num_assets = df_cap.shape[1]+1
bond_val = df_cap.iloc[0,0]+df_cap.iloc[0,2]-df_cap.iloc[0,1]

# add the bond to initial assets
init_asset_vals = np.hstack((np.array([bond_val]),
                            risky_asset_vals[0,:]))

bond_val

"""Make the initial portfolio"""

# consider here two choices: equal or equally-weighted 
init_port_choice =  'equal' 

init_cash = 1000.0
init_total_asset = np.sum(init_asset_vals)

x_vals_init = np.ones((4,))

'''
if init_port_choice == 'equal': 
    # hold equal amounts of cash in each asset
    amount_per_asset = init_cash/num_assets
    x_vals_init = amount_per_asset * np.ones(num_assets)

elif init_port_choice == 'equally_weighted':
    amount_per_asset = init_cash/init_total_asset
    x_vals_init = amount_per_asset * init_asset_vals
    '''

x_vals_init

"""Make the target portfolio

Define model parameters
"""

riskfree_rate = r
fee_bond = 0.00 # 0.01 # 
fee_stock = 0.05 # 0.05 # 20.0 # 0.1 # 1.0 # 100 # 1.0 # 0.5 

all_fees = np.zeros(num_assets)
all_fees[0] = fee_bond
all_fees[1:] = fee_stock
Omega_mat = np.diag(all_fees)

# model parameters
lambd = 0.001 
Omega_mat = 15.5 * np.diag(all_fees) 
eta = 1.5 

beta = 100.0
gamma = 0.95 
Sigma_r = cov_mat_torch

"""Simulate portfolio data"""

signals=z

W = torch.ones(num_assets, signals.shape[1], dtype=torch.float64, requires_grad=True)

W.shape

lambd = 0.001 
#omega = 1.0 
beta = 1000.0
eta = 1.5 # 1.3 # 1.5 # 1.2
rho = 0.4
#c=0.02

reward_params=[lambd, eta]

# Create a G-learner
num_risky_assets=num_assets-1
expected_risky_returns=risky_asset_returns

G_learner = G_learning_portfolio_opt(num_steps,           # numero di step della policy
                                     reward_params,
                                     W,                   # parametri del G-learner
                                                          # reward_params=[lambd, eta]
                                                          # lambd: avversione al rischio
                                                          # eta: parametro da applicare ai costi di transazione
                                     beta,                # coefficiente penalizzazione 
                                     gamma,               # fattore di attualizzazione intertemporale "non finanziario" 
                                     num_risky_assets,    # numero di strumenti diversi dal risk free
                                     expected_risky_returns,         # matrice dei rendimenti degli n asset rischiosi
                                     Sigma_r,             # covariance matrix of returns of risky assets
                                     x_vals_init,         # valore iniziale in euro della posizione negli n asset rischiosi
                                     signals,             # la matrice dei segnali
                                     riskfree_rate)
                                     #,
                                     #use_for_WM = True)



G_learner.reset_prior_policy()
error_tol=1.e-8 
max_iter_RL = 200
G_learner.G_learning(error_tol, max_iter_RL)

num_sim = 100
trajs = []
np.random.seed(0)
torch.manual_seed(0)
t_0 = time.time()


x_vals = [x_vals_init]
returns_all = []
for n in range(num_sim):
    this_traj = []
    x_t = x_vals_init[:]
    returns_array = []
    for t in range(0,num_steps):
        mu_t = G_learner.u_bar_prior[t,:] + G_learner.v_bar_prior[t,:].mv(torch.tensor(x_t))
        u_t = np.random.multivariate_normal(mu_t.detach().numpy(), G_learner.Sigma_prior[t,:].detach().numpy())
        # compute new values of x_t

        x_next = x_t +u_t
        # grow this with random return
        
        idiosync_vol =  0.00001 # vol_market (noise)     
        rand_norm = np.random.randn(num_risky_assets)*idiosync_vol
        
        # asset returns are simulated from a one-factor model
        risky_asset_returns = expected_risky_returns[t,:] + rand_norm
        
        returns = np.hstack((riskfree_rate, risky_asset_returns))
        
        x_next = (1+returns)*x_next
        port_returns=(x_next.sum() -x_t.sum() -np.sum(u_t) - 0.015*np.abs(u_t).sum())/x_t.sum()
        
        this_traj.append((x_t, u_t))
        
        # rename
        x_t = x_next
        returns_array.append(port_returns) 
    # end the loop over time steps
    trajs.append(this_traj)
    returns_all.append(returns_array)

print('Done simulating trajectories in %f sec'% (time.time() - t_0))

"""Calculate performance of G-learner (Diagnostics only)"""

returns_all_G = returns_all

SR_G = 0
for i in range(num_sim):
    SR_G += (np.mean(returns_all_G[i])-riskfree_rate)/np.std(returns_all_G[i])

SR_G/=num_sim
print(SR_G)

-0.08632798115042445

r_G = np.array([0]*num_steps, dtype='float64')
for n in range(num_steps):
    for i in range(num_sim):
        r_G[n]+=returns_all_G[i][n]
    r_G[n]/=num_sim

plt.plot(r_G, label='G-learning (' + str(np.round(SR_G,3)) + ')', color='red')

plt.xlabel('time (quarters)')
plt.ylabel('Sample Mean Returns');
